{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import initializers\n",
    "from keras.layers import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    " \n",
    "\n",
    "from keras.optimizers import Adam, Adagrad, Adadelta\n",
    "\n",
    "import keras.backend as bK\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# import statsmodels.formula.api as smf\n",
    "# import statsmodels.api as sm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(123)\n",
    "from tensorflow import set_random_seed\n",
    "from tensorflow.contrib.layers import fully_connected \n",
    "import tensorflow as tf\n",
    "set_random_seed(234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25706, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\jander33\\Desktop\\projects\\project2\\Production\\raw data\\WC_data\\WC Data West only.csv')\n",
    "df = data[data['New-Renew Ind'] != 'New']\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = df['Prime Agent Pd']\n",
    "y = df['New-Renew Ind']\n",
    "y = pd.get_dummies(y, drop_first = True)\n",
    "labels = np.asarray(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer() \n",
    "\n",
    "token.fit_on_texts(test)\n",
    "\n",
    "sequences = token.texts_to_sequences(test) # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55, 119], [714, 5, 3]]\n",
      "0    WESTERN ASSURANCE\n",
      "1    DALTON AGENCY INC\n",
      "Name: Prime Agent Pd, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# compare, just note that the cleaned data is now a keras object, Junk is not cleaned\n",
    "print(sequences[0:2]) # this is a list\n",
    "print(test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1237 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = token.word_index # type = dict\n",
    "print('Found {} unique tokens.'.format(len(word_index)))\n",
    "# max features = 1272 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,   55,  119],\n",
       "       [   0,    0,    0,  714,    5,    3],\n",
       "       [   0,    0, 1172, 1173,    2,    3],\n",
       "       [   0,    0,  283,    1,    8,    4]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen = None) \n",
    "#numpyarray of dim (samples, maxlen)\n",
    "data[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25706, 6)\n",
      "Shape of label tensor: (25706, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_word_index = {v+3:k for k,v in word_index.items()}\n",
    "rev_word_index[0] = 'padding_char'\n",
    "rev_word_index[1] = 'start_char'\n",
    "rev_word_index[2] = 'oov_char'\n",
    "rev_word_index[3] = 'unk_char'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'padding_char padding_char webb fc oov_char unk_char'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_feat = ' '.join([rev_word_index[word] for word in data[2]])\n",
    "example_feat #notice the zeros -> this is the padding. All lowerc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels,\n",
    "                                                    test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17994, 6)\n",
      "(7712, 6)\n",
      "(7712, 1)\n",
      "(17994, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,  15, 122, 433,   2],\n",
       "       [  0,   0,   0, 241, 206,   4],\n",
       "       [  0,   0, 281,  21,   1,   3]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1238"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = max([max(x) for x in X_train] + \n",
    "                   [max(x) for x in X_test]) + 1\n",
    "max_features # 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "bK.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(max_features, 64))\n",
    "# model.compile('rmsprop', 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1238"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_i = Input(shape=(6, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape = batches of 6 dimensional vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 6  # 6 dimensional input\n",
    "num_hidden = 1  # 1 dimensional representation \n",
    "num_outputs = num_inputs # Must be true for an autoencoder!\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers\n",
    "hidden = fully_connected(X, num_hidden, activation_fn=None)\n",
    "outputs = fully_connected(hidden, num_outputs, activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "loss = tf.reduce_mean(tf.square(outputs - X))  # MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train  = optimizer.minimize( loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for iteration in range(num_steps):\n",
    "        sess.run(train,feed_dict={X: data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "        \n",
    "    # Now ask for the hidden layer output (the 2 dimensional output)\n",
    "    output_1d = hidden.eval(feed_dict={X: data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -64.22485352],\n",
       "       [  547.69403076],\n",
       "       [ 1281.24291992],\n",
       "       [   93.71153259],\n",
       "       [  554.74285889],\n",
       "       [  185.24230957],\n",
       "       [ -222.40309143],\n",
       "       [  151.62176514],\n",
       "       [  -11.93527603],\n",
       "       [  449.03607178],\n",
       "       [   42.47973251],\n",
       "       [  146.42036438],\n",
       "       [  165.66860962],\n",
       "       [  665.9586792 ],\n",
       "       [   -5.61925697],\n",
       "       [   54.179245  ],\n",
       "       [   79.79898071],\n",
       "       [  176.08056641]], dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1d[0:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary), 300))\n",
    "model.compile('rmsprop', 'mse')\n",
    "\n",
    "input_i = Input(shape=(30, 300))\n",
    "encoded_h1 = Dense(64, activation='tanh')(input_i)\n",
    "encoded_h2 = Dense(32, activation='tanh')(encoded_h1)\n",
    "encoded_h3 = Dense(16, activation='tanh')(encoded_h2)\n",
    "encoded_h4 = Dense(8, activation='tanh')(encoded_h3)\n",
    "encoded_h5 = Dense(4, activation='tanh')(encoded_h4)\n",
    "latent = Dense(2, activation='tanh')(encoded_h5)\n",
    "decoder_h1 = Dense(4, activation='tanh')(latent)\n",
    "decoder_h2 = Dense(8, activation='tanh')(decoder_h1)\n",
    "decoder_h3 = Dense(16, activation='tanh')(decoder_h2)\n",
    "decoder_h4 = Dense(32, activation='tanh')(decoder_h3)\n",
    "decoder_h5 = Dense(64, activation='tanh')(decoder_h4)\n",
    "\n",
    "output = Dense(300, activation='tanh')(decoder_h5)\n",
    "\n",
    "autoencoder = Model(input_i,output)\n",
    "\n",
    "autoencoder.compile('adadelta','mse')\n",
    "\n",
    "X_embedded = model.predict(X_train)\n",
    "autoencoder.fit(X_embedded,X_embedded,epochs=10, batch_size=256, validation_split=.1)\n",
    "\n",
    "print autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(max_features, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_1 to have shape (1238,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-b7839d02778c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m           validation_split=0.3)\n\u001b[0m",
      "\u001b[1;32mC:\\Miniconda3\\envs\\tf\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1591\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1592\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1593\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1594\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1595\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1431\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1432\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    118\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have shape (1238,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=20,\n",
    "          callbacks=[EarlyStopping(monitor='acc', patience=1)],\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dict mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_dict(col, dframe):\n",
    "    \n",
    "    '''returns a dict for nominals. This can then be mapped to \n",
    "    any col to create coded feature'''\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    \n",
    "    #df = pd.DataFrame(dframe[col].value_counts())\n",
    "    #df.reset_index(level=0, inplace=True)\n",
    "    #df.rename(columns={'index': 'key', col: 'value'}, inplace=True)\n",
    "#     df_dict = defaultdict(list)\n",
    "#     for k, v in zip(df.key, df.value):\n",
    "#         df_dict[k] = (int(v))\n",
    "    return df_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "UW2rank = {'Mulloy,Spencer': 23,       \n",
    "'Despain,Royd': 22,         \n",
    "'Rodlin,Debbie': 21,      \n",
    "'Armstrong,Ryan': 20,       \n",
    "'Moralez,Randa': 19,        \n",
    "'Dunn,Jonathan': 18,        \n",
    "'Carmicheal,Debra': 17,     \n",
    "'Chaudhuri,Debbie': 16,     \n",
    "'Fuss,Amanda': 15,        \n",
    "'Reeves,Heather': 14,      \n",
    "'Harding,Kristine': 13,      \n",
    "'Harris,Norm': 12,          \n",
    "'Ruggeberg,Mike': 11,      \n",
    "'Moore,David': 10,         \n",
    "'Hilst,Scott': 9,            \n",
    "'Furness,Nick': 8,          \n",
    "'White,Becky': 7,           \n",
    "'Dole,Kylie': 6,           \n",
    "'Denney,Melanie': 5,         \n",
    "'Eastwood,Jennifer': 4,      \n",
    "'Costello,Cathleen': 3,      \n",
    "'Sebolt,Catie': 2,           \n",
    "'Rybeck,Chris': 1,           \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(col, dframe):\n",
    "    \n",
    "    '''returns a dict for freqs. This can then be mapped to \n",
    "    any col to create freq feature'''\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    \n",
    "    df = pd.DataFrame(dframe[col].value_counts())\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df.rename(columns={'index': 'key', col: 'value'}, inplace=True)\n",
    "    df_dict = defaultdict(list)\n",
    "    for k, v in zip(df.key, df.value):\n",
    "        df_dict[k] = (int(v))\n",
    "    return df_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_group(freq, _dict, rare, infrequent, less_common):\n",
    "    \n",
    "    rev_dict = {v:k for k, v in _dict.items()}\n",
    "    \n",
    "    if freq <= rare:\n",
    "        string = 'rare'\n",
    "    elif freq > rare and freq <= infrequent:\n",
    "        string = 'infrequent'\n",
    "    elif freq > infrequent and freq <= less_common:\n",
    "        string = 'less common'\n",
    "    else:\n",
    "        string = rev_dict[freq]\n",
    "    return(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy lifetime feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_life =  pd.concat([df['Effective YearMonth'].apply(lambda x: str(x)), \n",
    "                   df['renew'], df['tenure'].apply(lambda x: str(x)),\n",
    "                   ], axis=1)\n",
    "pol_life = pol_life.apply(lambda x: ' '.join(x), axis=1) \n",
    "\n",
    "period = datetime(year=2018, month=7, day=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "â€‹def delta(string, period):\n",
    "    \n",
    "    ''' The following is needed before running this:\n",
    "    \n",
    "        pol_life =  pd.concat([df['Effective YearMonth'].apply(lambda x: str(x)), \n",
    "                   df['renew'], df['tenure'].apply(lambda x: str(x)),\n",
    "                   ], axis=1)\n",
    "                   \n",
    "        pol_life = pol_life.apply(lambda x: ' '.join(x), axis=1) \n",
    "        period = datetime(year=2018, month=7, day=1) \n",
    "        \n",
    "        nan are good in this output. The function grabs the most recent renewal for a \n",
    "        certain line account and nans the other ones.'''\n",
    "    \n",
    "    eff = datetime.strptime(string.split()[0],'%Y%m')\n",
    "    tenure = int(float(string.split()[2]))\n",
    "    while string.split()[1] == 'Renew' and eff > period.replace(year=int(period.year - 1)):\n",
    "        \n",
    "        '''Take it back two years. Most of the violaters have more tenure.\n",
    "        Renews with tenure = 1 are over 1 year in lifetime. The nans are a good thing. \n",
    "        They are the prior renewals on the same policy'''\n",
    "            \n",
    "        for i in range(1, 32):\n",
    "            #print(i)\n",
    "            # start with tenure 2\n",
    "\n",
    "            if tenure == i:\n",
    "                delta = period - eff\n",
    "                days = delta.days + 365*i\n",
    "                return(days)\n",
    "            else:\n",
    "                continue                \n",
    "\n",
    "        break\n",
    "        \n",
    "    while string.split()[1] == 'New':\n",
    "        \n",
    "        if eff >= period.replace(year=int(period.year - 1)):\n",
    "            delta = period - eff\n",
    "            days = delta.days\n",
    "            return(days)\n",
    "    \n",
    "        else: \n",
    "            days = 'remove'\n",
    "            return(days)\n",
    "        break\n",
    " \n",
    "\n",
    "    while string.split()[1] == 'Non-Renew':\n",
    "        \n",
    "        days = tenure*365\n",
    "        return(days)\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life = pol_life.apply(lambda x: delta(x, period))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
